--PULL strategy
    -connecting to relational databases/any system to pull the data.
    -This might cause issues as it might overload the systems. i.e., if we are using sqoop to load data from a OLTP database, it might overload the OLTP database.
--PUSH strategy 
    -It will be source systems responsibility to provide the data. generally source systems will push the data into staging file servers and from there we will load into hadoop. 
    -This is the most preferred way for OLTP systems as if we use pull strategy, there are chances that the developer might use more mappers to increase parallelism and cause overload on the critical OLTP systems.
    -Generally interaction with Mainframe is not that easy and mainframe teams provide the files in the requested format onto the file servers
    -Third party applications and middle ware technologies like tibco, mq provide the data onto the staged systems.
--SQOOP is generally used for PULL strategy and MR, HIVE , SPARK .. can be used for PUSH strategy
--Mainframes uses IBM NDM to push the data into staging servers/locations

-- filter the data as much as possible and as early as possible
--HIVE is used to load data from flat files into HDFS, MR can extend the capabilities of load process
-------------------------------------------------------------------------------------------------------
SQOOP 
    -- data load tool, both in(import) and out of(export) hadoop
    -- written in Java and open source
    -- Uses JDBC for DB connectivity
    -- Uses Map Reduce framework
    -- sqoop follows map only architecture, so no reduce tasks in import & export
    -- sqoop is bunch of jar files which does the required tasks
    -- it is not that secure
    -- only 3 components in sqoop architecture - source systems, hadoop and sqoop 
    -- only command line interface
    -- Map task connects to both DBs and hadoop
    
SQOOP2
    --It uses Client/Server architecture
    --It uses both map and reduce
    --avaiable on web and command line
    --it is highly secure
    --Admins setup the connectors to the database and developers use the connector ids to connect
    --Sqoop client(web browser/command line) will connect to the SQOOP Server(this supports REST/UI and has the connectors and metadata) and SQOOP Server is connected to Metadata repository , HDFS/HIVE/HBASE(through reduce task) and DBs(through map task).
    

HIVE(Load)
    --Files format should match with the table's format - file type(text, sequence ,...) and delimiters(both line and field) should be consistent
    --Load does not use mapreduce, it uses file system APIs to copy files from local file system/HDFS to HDFS
    --LOAD DATA [LOCAL] INPATH <filepath> [OVERWRITE] INTO TABLE <tablename> [PARTITION(partcol1=val1, partcol2=val2 ...)]
      PARTITION will specify to which partition the data has to be copied
      ex: LOAD DATA LOCAL INPATH '/home/cloudera/test.txt' OVERWRITE INTO TABLE test;
      
  Sample Load:
    --create a text file - ex., words.txt - with single word in each line, that will match to the single column
    --then create a table with single column - "create table test(t string);"
    --now load the data - "load data local inpath '/user/cloudera/words.txt' INTO TABLE test;"
    --check the results - "select * from test;"

 ------------------------------------------------------------------------------------------------------------------------
 --To demostrate hive load we have to create text files but as it is time consuming to create text files, we will get data from sql database and then load it to hive using hive load, in real world, we will receive these files from upstreams
    use retail_ods;
    --use mysql to directly write the file, run this command in sql prompt and make sure that user have permissions to write onto the disk 
    select * from categories into outfile '/tmp/categories.psv' fields terminated by '|' lines terminated by '\n';
    --the above command might fail if user does not have required permissions
    --then login as root
    mysql -u root -p
    select user from mysql.user;
    describe mysql.user;
    -- File_priv varaiable is by default 'N' 
    select user,File_priv from mysql.user;
    update mysql.user set file_priv ='Y' where user = 'retail_dba';
    commit;
    exit;
    -- we have to restart the service for the file permissions to get set properly
    service mysqld restart
    --login as retail_dba and run the command, psv is pipe seperated file format extension
    select * from categories into outfile '/tmp/temp.psv' fields terminated by '|' lines terminated by '\n';
    select * from categories into outfile '/tmp/temp.csv' fields terminated by ',' lines terminated by '\n';
    
    --In hive, delete data from categories table and then load the data
    load data local inpath '/tmp/temp.csv' into table categories ;
    --now read the data and we will see all NULL due to the delimiter mismatch but data will be present in the hive direcotry
    select * from categories;
    dfs -cat /user/hive/warehouse/categories/* ;
    --now load the data with correct format ino the hive table
    load data local inpath '/temp/temp.psv' overwrite into table categories;
    --now we can see the data
    select * from categories;
    -- to append the data, we have to rerun the command without overwrite keyword,  a new file will be created in hdfs table directory with the data along with the existing file.
    -- If we load the data from HDFS path to hive table, if the user has correct permissions, the HDFS file will be deleted after successful load
    hdfs dfs -mkdir /user/cloudera/categories
    hdfs dfs -put /tmp/temp.psv /user/cloudera/categories/.
    hdfs dfs -ls /user/cloudera/categories/*
    load data inpath  /user/cloudera/categories/temp.psv overwrite into table categories;
    --Now if we check the file will be deleted from hdfs
    hdfs dfs -ls /user/cloudera/categories/*
    
-If the hive table has more columns or if partitioning exists on the tables or if there are file format changes we cannot use the HIVE LOAD, we have to use INSERT in this case.
    
HIVE INSERT
    --Insert command is used to select data from one table to another table or partition of another table.
    --we can apply transformations, data cleansing, filtering etc.
    --the source and destination tables can be in different formats.
    --in INSERT OVERWRITE old files will be deleted
    --INSERT INTO will not delete the old files, it simulates append
    --An output of a query should be inserted into the table
    --we can insert into bucketed tables and also partitioned tables(both dynamic and static)
    --hive.exec.dynamic.partition needs to be set to 'TRUE' to enable dynamic partition inserts, default is FALSE
    --hive.exec.dynamic.partition.mode - In 'strict' mode, user must specify at least one static partition in case the user overwrites all partitions, in 'nonstrict' mode all partitions are allowed to be dynamic. default is 'strict'
    --hive.exec.max.dynamic.partitions.pernode - maximum number of dynamic partitions allowed to be created in each mapper/reducer node,  default is 100
    --hive.exec.max.dynamic.partitions - maximum number of dynamic partitions allowed to be created in total, default is 1000
    --hive.exec.max.created.files - maximum number of HDFS files created by all mappers/reducers in MapReduce job, default is 100000
    --hive.error.on.empty.partition - whether to throw an exception if dynamic partition insert generates empty results, default is FALSE
    
Differences between HIVE LOAD and HIVE INSERT:
    --LOAD will use file system APIs, while INSERT uses map reduce jobs/tej/spark
    --LOAD is to copy files from local/HDFS to HDFS , while insert is to copy from one table to another
    insert overwrite table products select * from products;

Differences between HIVE LOAD/INSERT Vs RDBMS
    --we cannot specify columns on insert/load
    --does not support update/delete, we have to load the delta and do outer joins if needed
    --schema on read, so we should be very carefull while writing the data, user has to make sure the schema matches while writing
    --no transactions, so no commit or redo log
    
--Data quality is generally taken care by hive, if it is not doing that as expected, we might have to do it with execution engines

--hive-execution-engine can be used to change execution engine, ex., mr

    -------------------------------------------------------------------------------------------------------------
    drop retail_ods_stage cascade;
    --retail_stage is the first place to load all the data from databases, from here it is loaded to ODS tables
    --schema of tables in retail_stage should match with the schema of sql tables
    
 First we will load from mysql to table under retail_stage database and from there we will load to retail_ods database
    select * from orders into outfile '/tmp/orders.psv' fields terminated by '|' lines terminated by '\n';
    use retail_stage;
    -- we are creating a staging table 
    create table orders_stage(order_id int, order_date string, order_customer_id int, order_status string) row format delimited fields terminated by '|'stored as textfile;
    load data local inpath '/tmp/orders.psv' into table orders_stage;
    select * from orders_stage limit 10;
    --we have to check the hive.exec.dynamic.partition and hive.exec.dynamic.partition.mode  properties, former one should be 'true' and the later one should be 'nonstrict' in this case.
    set hive.execute.dynamic.partition=true;
    set hive.execute.dynamic.partition.mode=nonstrict;
    --now check if there is any data in ods table and load into ods table
    insert overwrite table retail_ods.orders partition(order_month) select order_id , order_date , order_customer_id , order_status, substr(order_date, 1,7) order_month from retail_stage.orders_stage
    
    --loading data for order_items table, there are two ways to load the data into the retail_ods.order_items table, we can do the join in sql, copy the file to stage db and load it or we can load the data into stage and join it while loading to ods, second way is the better as join happends in hive and hadoop
    -- check if the data exists in the order_items table, if not load it from sql db using hive load.
    
    use retail_ods;
    insert overwrite table order_items partition(order_month) select oi.order_item_id, oi.order_item_order_id, o.order_date, oi.order_item_product_id, oi.order_item_quantity, oi.order_item_subtotal, oi.order_item_product_price, substr(o.order_date,1,7) order_month from retail_stage.orders_stage o join retail_stage.order_items oi on o.order_id = oi.order_item_order_id; 
    select count(1) from order_items;
    select * from order_items limit 10;
    
 
    
    

                   
