#first we have to import the SQLContext and Row class
from pyspark.sql import SQLContext, Row
sqlContext=SQLContext(sc)

# setting the shuffle partitions to 10 as the default is 200 and it will fire 200 tasks, which is not needed for our data size
sqlContext.sql("set spark.sql.shuffle.partitions=10");

#read the data from orders text files on hdfs
ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
# run the map function to split the data
ordersMap = ordersRDD.map(lambda a: a.split(","))
# define the schema
orders= ordersMap.map(lambda o: Row(order_id=int(o[0]), order_date =o[1] , order_customer_id=int(o[2]), order_status=o[3]))
#infer the schema
ordersSchema = sqlCtx.inferSchema(orders)
#register the table
ordersSchema.registerTempTable("orders")

#now read the order_items table similar to the orders table file
orderItemsRDD=sc.textFile("/user/cloudera/sqoop_import/order_items")
orderItemsMap=orderItemsRDD.map(lambda x:x.split(","))
# please note that we are defining only the columns that we need, missing 4th column and last column
orderItems=orderItemsMap.map(lambda o:Row(order_items_id=int(o[0]), order_item_order_id=int(o[1]), order_item_product_id =int(o[2]) , order_item_subtotal=float(o[4]) ))
orderItemsSchema = sqlCtx.inferSchema(orderItems)
orderItemsSchema.registerTempTable("order_items")

# as both the tables are registered now, we can run the same query we ran with hive 
joinData = sqlCtx.sql("select o.order_date, sum(oi.order_item_subtotal), count(distinct oi.order_item_order_id) from orders o join order_items oi on o.order_id=oi.order_item_order_id group by o.order_date order by o.order_date")


